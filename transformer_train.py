# -*- coding: utf-8 -*-
"""
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11kSVByFlnxup7Y8nZwfv-nH-zj3RS4zD

##### Copyright 2022 The TensorFlow Authors.

# Neural machine translation with a Transformer and Keras

"""
import sys
import os
from dotenv import load_dotenv

default_config_file = None  #os.path.abspath("config.env")

if len(sys.argv) > 1:
    config_file = os.path.abspath(sys.argv[1])
    load_dotenv(config_file,override=True)   #looks for the specified .env file and loads variables (overiding any previous set env variables)
else:
    config_file = default_config_file
    # if no config file passed as argument then it uses the active environment variables else it uses the default values specified in the source code



import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf

import tensorflow_text
import random
import nltk.translate.bleu_score as bleu

print("\n---------------TRANSLATEKAR - A NEURAL MACHINE TRANSLATOR-----------------------\n")

print(f"Configuration file: {config_file}\n")

MODEL_NAME=os.environ['MODEL_NAME']
path_to_target = os.path.abspath(os.environ['TARGET_DATA_PATH'])
path_to_context = os.path.abspath(os.environ['CONTEXT_DATA_PATH'])
model_dir = os.path.abspath(f'{MODEL_NAME}')

# os.environ['MODEL_NAME']=MODEL_NAME
os.environ['MODEL_PATH']=model_dir

#importing legal dataset vocabulary
context_vocab_path=os.path.abspath(os.environ['CONTEXT_TOKEN_PATH'])
target_vocab_path=os.path.abspath(os.environ['TARGET_TOKEN_PATH'])

print("Model Name:",MODEL_NAME)
print("Context Dataset:",path_to_context)
print("Target Dataset:",path_to_target)
print("Context Vocab:",context_vocab_path)
print("Target Vocab:",target_vocab_path)
print()

#create model directory
os.makedirs(model_dir,exist_ok=True)

# Model configurations

BATCH_SIZE = int(os.environ['BATCH_SIZE']) if 'BATCH_SIZE' in os.environ else 64
MAX_TOKENS = int(os.environ['MAX_TOKENS']) if 'MAX_TOKENS' in os.environ else 128

num_layers = int(os.environ['NUM_LAYERS']) if 'NUM_LAYERS' in os.environ else 4
d_model = int(os.environ['D_MODEL']) if 'D_MODEL' in os.environ else 128
dff = int(os.environ['DFF']) if 'DFF' in os.environ else 512
num_heads = int(os.environ['NUM_HEADS']) if 'NUM_HEADS' in os.environ else 8
dropout_rate = float(os.environ['DROPOUT_RATE']) if 'DROPOUT_RATE' in os.environ else 0.1

print("Model configurations\n")
print("Batch size:",BATCH_SIZE)
print("Maximum Token length:",MAX_TOKENS)
print("Number of layers (encoder and decoder):",num_layers)
print("Dimensionality of the embeddings",d_model)
print("Internal dimensionality of the FeedForward layer:",dff)
print("Number of self-attention heads",num_heads)
print("Dropout rate",dropout_rate)
print()

#Transformer mini configs
#num_layers = 4
#d_model = 128   #dimensionality of the embeddings
#dff = 512       #internal dimensionality of the FeedForward layer
#num_heads = 8   #The number of self-attention heads remains the same
#dropout_rate = 0.1

#base paper configuration
#num_layers=6, d_model=512, dff=2048, num_heads=8, dropout_rate=0.1

# Training parameters
epochs = int(os.environ['epochs']) if 'epochs' in os.environ and int(os.environ['epochs']) >=1 else 10
save_freq = int(os.environ['save_freq']) if 'save_freq' in os.environ and int(os.environ['save_freq']) >1 else 'epoch'

#save_weights_only = False if os.environ.get("save_weights_only") in ['False','false','FALSE'] else True
save_weights_only = True
save_best_only = False if os.environ.get("save_best_only") in ['False','false','FALSE'] else True

print("Training Parameters\n")
print("Epochs:",epochs)
print("save_weights_only:",save_weights_only)
print("save_best_only:",save_best_only)
print("save_freq:",save_freq)
print()

# Define a callback to save checkpoints
if save_best_only == False:
  if save_weights_only == True:
    checkpoint_filepath = os.path.join(model_dir,'checkpoints/weights.{epoch:02d}-{val_loss:.4f}.hdf5')
  else:
    checkpoint_filepath = os.path.join(model_dir,'checkpoints/model.{epoch:02d}-{val_loss:.4f}.hdf5')
elif save_weights_only == False:
  checkpoint_filepath = os.path.join(model_dir,'checkpoints/best_model.hdf5')
else:
  checkpoint_filepath = os.path.join(model_dir,'checkpoints/best_model.weights.hdf5')

# Create the directory if it doesn't exist
os.makedirs(os.path.join(model_dir,'checkpoints'), exist_ok=True)

print("Checkpoint path:",checkpoint_filepath)
print()

pretrained_weights = os.path.abspath(os.environ['WEIGHTS_PATH']) if 'WEIGHTS_PATH' in os.environ and os.environ['WEIGHTS_PATH'].lower() not in ['none','n',''] else None
print("Pretrained weights:",pretrained_weights)
print()


#load corpus from text file into a numpy list
def load_data(path):

    with open(path, encoding='utf-8') as file:
        lines = file.readlines()

        len_data=len(lines)
        context = np.empty(shape=len_data,dtype=object)
        i=0
        for line in lines:
            context[i] = line.strip()
            i+=1
    del lines
    return context
#----------------------------------------------

#splits corpus into 2 parts of specified ratio
def split_corpus(context_full_raw,target_full_raw,ratio):
  is_train_val = np.random.uniform(size=(len(target_full_raw),)) < ratio

  context_test = context_full_raw[~is_train_val]
  target_test = target_full_raw[~is_train_val]
  context_data = context_full_raw[is_train_val]
  target_data = target_full_raw[is_train_val]

  NUM_SAMPLES = len(context_data)
  NUM_SAMPLES_0 = len(context_test)

  return context_data,target_data,context_test,target_test,NUM_SAMPLES,NUM_SAMPLES_0
#----------------------------------------------

#save test/train data to file
def save_corpus_to_file(model_dir,source_filename,target_filename,context,target):
  with open(os.path.join(model_dir,source_filename),'w',encoding='utf-8') as src:
      for src_line in context.tolist():
        src.write(src_line+'\n')

  with open(os.path.join(model_dir,target_filename),'w',encoding='utf-8') as trg:
      for trg_line in target.tolist():
        trg.write(trg_line+'\n')
  print(os.listdir(model_dir))
#----------------------------------------------

#shuffle dataset
def shuffle_corpus(context,target):

  zipped = list(zip(context, target))
  random.shuffle(zipped)
  context,target = list(zip(*zipped))

  return np.array(list(context)),np.array(list(target))
#----------------------------------------------

#load data
target_full_raw = load_data(path_to_target)
context_full_raw = load_data(path_to_context)

#split corpus to train and test
context_data,target_data,context_test,target_test,num_train_val_samples,num_test_samples = split_corpus(context_full_raw,target_full_raw,0.998)
print("length test:",num_test_samples)

#split train data to train and validation
context_train,target_train,context_val,target_val,num_train_samples,num_val_samples = split_corpus(context_data,target_data,0.998)
print("length train:",num_train_samples)
print("length validation:",num_val_samples)

print("Context train[:5]\n",context_train[:5])
print("Target train[:5]\n",target_train[:5])
print()

#save validation data to file
save_corpus_to_file(model_dir,"src.val.txt","trg.val.txt",context_val,target_val)
#save test data to file
save_corpus_to_file(model_dir,"src.test.txt","trg.test.txt",context_test,target_test)

#convert and pair context and target of train and val data to tensor

BUFFER_SIZE = num_train_samples

train_examples = (
    tf.data.Dataset
    .from_tensor_slices((context_train, target_train))
    .shuffle(BUFFER_SIZE)
    )
val_examples = (
    tf.data.Dataset
    .from_tensor_slices((context_val, target_val))
    .shuffle(BUFFER_SIZE)
    )

#convert context and target dataset to tensor
full_context = tf.data.Dataset.from_tensor_slices(context_full_raw)
full_target = tf.data.Dataset.from_tensor_slices(target_full_raw)


"""### ***TOKENIZING KONKANI SENTENCES***"""

from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab

bert_tokenizer_params=dict(lower_case=False)
reserved_tokens=["[PAD]", "[UNK]", "[START]", "[END]"]

bert_vocab_args = dict(
    # The target vocabulary size
    vocab_size = 100000,
    # Reserved tokens that must be included in the vocabulary
    reserved_tokens=reserved_tokens,
    # Arguments for `text.BertTokenizer`
    bert_tokenizer_params=bert_tokenizer_params,
    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`
    learn_params={},
)

#open the english and konkani vocabulary
context_vocab = load_data(context_vocab_path)
target_vocab = load_data(target_vocab_path)
context_vocab_size = len(context_vocab)
target_vocab_size = len(target_vocab)

print("Length of Context vocab:",context_vocab_size)
print("Examples:")
print(context_vocab[:10])
print(context_vocab[100:110])
print()
print("length of Target vocab:",target_vocab_size)
print("Examples:")
print(target_vocab[:10])
print(target_vocab[100:110])
print()

context_tokenizer = tensorflow_text.BertTokenizer(context_vocab_path, **bert_tokenizer_params)
target_tokenizer = tensorflow_text.BertTokenizer(target_vocab_path, **bert_tokenizer_params)

START = tf.argmax(tf.constant(reserved_tokens) == "[START]")
END = tf.argmax(tf.constant(reserved_tokens) == "[END]")

def add_start_end(ragged):
    count = ragged.bounding_shape()[0]
    starts = tf.fill([count,1], START)
    ends = tf.fill([count,1], END)
    return tf.concat([starts, ragged, ends], axis=1)
#----------------------------------------------
def tokenize(inputs,tokenizer):
    token_batch = tokenizer.tokenize(inputs)
    token_batch = token_batch.merge_dims(-2,-1)
    return add_start_end(token_batch)
#----------------------------------------------

def prepare_batch(ct, tg):

    ct = tokenize(ct,context_tokenizer)
    ct = ct[:, :MAX_TOKENS]    # Trim to MAX_TOKENS.
    ct = ct.to_tensor()  # Convert to 0-padded dense Tensor

    tg = tokenize(tg,target_tokenizer)
    tg = tg[:, :(MAX_TOKENS+1)]
    tg_inputs = tg[:, :-1].to_tensor()  # Drop the [END] tokens
    tg_labels = tg[:, 1:].to_tensor()   # Drop the [START] tokens

    return (ct, tg_inputs), tg_labels
#----------------------------------------------

"""The function below converts a dataset of text examples into data of batches for training.

1. It tokenizes the text, and filters out the sequences that are too long.
   (The `batch`/`unbatch` is included because the tokenizer is much more efficient on large batches).
2. The `cache` method ensures that that work is only executed once.
3. Then `shuffle` and, `dense_to_ragged_batch` randomize the order and assemble batches of examples.
4. Finally `prefetch` runs the dataset in parallel with the model to ensure that data is available when needed. See [Better performance with the `tf.data`](https://www.tensorflow.org/guide/data_performance.ipynb) for details.
"""

def make_batches(ds):
  return (
      ds
      .shuffle(BUFFER_SIZE)
      .batch(BATCH_SIZE)
      .map(prepare_batch, tf.data.AUTOTUNE)
      .prefetch(buffer_size=tf.data.AUTOTUNE))
#----------------------------------------------

# Create training and validation set batches.
train_batches = make_batches(train_examples)
val_batches = make_batches(val_examples)

for (ct, tg), tg_labels in train_batches.take(1):
  break

print("Shape Target:",tg.shape)
print("Shape Target labels:",tg_labels.shape)

"""The `tg` and `tg_labels` are the same, just shifted by 1:"""


"""## Define the components

### The embedding and positional encoding layer
"""

print("\nDefining the components:")

def positional_encoding(length, depth):
  depth = depth/2

  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)
  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)

  angle_rates = 1 / (10000**depths)         # (1, depth)
  angle_rads = positions * angle_rates      # (pos, depth)

  pos_encoding = np.concatenate(
      [np.sin(angle_rads), np.cos(angle_rads)],
      axis=-1)

  return tf.cast(pos_encoding, dtype=tf.float32)
#----------------------------------------------

"""The position encoding function is a stack of sines and cosines that vibrate at different frequencies depending on their location along the depth of the embedding vector. They vibrate across the position axis."""

#@title
pos_encoding = positional_encoding(length=2048, depth=512)

# Check the shape.
print("Shape Positional encoding:",pos_encoding.shape)

"""So use this to create a `PositionEmbedding` layer that looks-up a token's embedding vector and adds the position vector:"""

class PositionalEmbedding(tf.keras.layers.Layer):
  def __init__(self, vocab_size, d_model):
    super().__init__()
    self.d_model = d_model
    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)
    self.pos_encoding = positional_encoding(length=2048, depth=d_model)

  def compute_mask(self, *args, **kwargs):
    return self.embedding.compute_mask(*args, **kwargs)

  def call(self, x):
    length = tf.shape(x)[1]
    x = self.embedding(x)
    # This factor sets the relative scale of the embedding and positonal_encoding.
    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
    x = x + self.pos_encoding[tf.newaxis, :length, :]
    return x
#----------------------------------------------

embed_context = PositionalEmbedding(vocab_size=context_vocab_size, d_model=512)
embed_target = PositionalEmbedding(vocab_size=target_vocab_size, d_model=512)

context_emb = embed_context(ct)
target_emb = embed_target(tg)

"""### The base attention layer"""

class BaseAttention(tf.keras.layers.Layer):
  def __init__(self, **kwargs):
    super().__init__()
    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)
    self.layernorm = tf.keras.layers.LayerNormalization()
    self.add = tf.keras.layers.Add()
#----------------------------------------------

"""#### Attention refresher

Before you get into the specifics of each usage, here is a quick refresher on how attention works:

### The cross attention layer

To implement this you pass the target sequence `x` as the `query` and the `context` sequence as the `key/value` when calling the `mha` layer:
"""

class CrossAttention(BaseAttention):
  def call(self, x, context):
    attn_output, attn_scores = self.mha(
        query=x,
        key=context,
        value=context,
        return_attention_scores=True)

    # Cache the attention scores for plotting later.
    self.last_attn_scores = attn_scores

    x = self.add([x, attn_output])
    x = self.layernorm(x)

    return x
#----------------------------------------------

"""Test run it on sample inputs:"""

sample_ca = CrossAttention(num_heads=2, key_dim=512)

print("Shape context embedding:",context_emb.shape)
print("Target context embedding:",target_emb.shape)
print("Shape sample CrossAttention:",sample_ca(target_emb, context_emb).shape)

"""### The global self attention layer

This layer is responsible for processing the context sequence, and propagating information along its length:
"""

class GlobalSelfAttention(BaseAttention):
  def call(self, x):
    attn_output = self.mha(
        query=x,
        value=x,
        key=x)
    x = self.add([x, attn_output])
    x = self.layernorm(x)
    return x
#----------------------------------------------

sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)

print("Shape context embedding",context_emb.shape)
print("Shape sample GlobalSelfAttention",sample_gsa(context_emb).shape)

"""### The causal self attention layer

This layer does a similar job as the global self attention layer, for the output sequence:
"""

class CausalSelfAttention(BaseAttention):
  def call(self, x):
    attn_output = self.mha(
        query=x,
        value=x,
        key=x,
        use_causal_mask = True)
    x = self.add([x, attn_output])
    x = self.layernorm(x)
    return x
#----------------------------------------------

"""Test out the layer:"""

sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)

print("Shape Traget Embedding:",target_emb.shape)
print("Shape sample CausalSelfAttention:",sample_csa(target_emb).shape)

"""The output for early sequence elements doesn't depend on later elements, so it shouldn't matter if you trim elements before or after applying the layer:"""

out1 = sample_csa(embed_target(tg[:, :3]))
out2 = sample_csa(embed_target(tg))[:, :3]

tf.reduce_max(abs(out1 - out2)).numpy()

"""Note: When using Keras masks, the output values at invalid locations are not well defined. So the above may not hold for masked regions.

### The feed forward network

The transformer also includes this point-wise feed-forward network in both the encoder and decoder:

The network consists of two linear layers (`tf.keras.layers.Dense`) with a ReLU activation in-between, and a dropout layer. As with the attention layers the code here also includes the residual connection and normalization:
"""

class FeedForward(tf.keras.layers.Layer):
  def __init__(self, d_model, dff, dropout_rate=0.1):
    super().__init__()
    self.seq = tf.keras.Sequential([
      tf.keras.layers.Dense(dff, activation='relu'),
      tf.keras.layers.Dense(d_model),
      tf.keras.layers.Dropout(dropout_rate)
    ])
    self.add = tf.keras.layers.Add()
    self.layer_norm = tf.keras.layers.LayerNormalization()

  def call(self, x):
    x = self.add([x, self.seq(x)])
    x = self.layer_norm(x)
    return x
#----------------------------------------------

"""Test the layer, the output is the same shape as the input:"""

sample_ffn = FeedForward(512, 2048)

print("Shape Target Embedding:",target_emb.shape)
print("Shape Sample FFN:",sample_ffn(target_emb).shape)

"""### The encoder layer

The encoder contains a stack of `N` encoder layers. Where each `EncoderLayer` contains a `GlobalSelfAttention` and `FeedForward` layer:

Here is the definition of the `EncoderLayer`:
"""

class EncoderLayer(tf.keras.layers.Layer):
  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):
    super().__init__()

    self.self_attention = GlobalSelfAttention(
        num_heads=num_heads,
        key_dim=d_model,
        dropout=dropout_rate)

    self.ffn = FeedForward(d_model, dff)

  def call(self, x):
    x = self.self_attention(x)
    x = self.ffn(x)
    return x
#----------------------------------------------

"""And a quick test, the output will have the same shape as the input:"""

sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)

print("Shape context Embedding:",context_emb.shape)
print("Shape sample encoder layer:",sample_encoder_layer(context_emb).shape)


"""### The encoder

Next build the encoder.

The encoder consists of:

- A `PositionalEmbedding` layer at the input.
- A stack of `EncoderLayer` layers.
"""

class Encoder(tf.keras.layers.Layer):
  def __init__(self, *, num_layers, d_model, num_heads,
               dff, vocab_size, dropout_rate=0.1):
    super().__init__()

    self.d_model = d_model
    self.num_layers = num_layers

    self.pos_embedding = PositionalEmbedding(
        vocab_size=vocab_size, d_model=d_model)

    self.enc_layers = [
        EncoderLayer(d_model=d_model,
                     num_heads=num_heads,
                     dff=dff,
                     dropout_rate=dropout_rate)
        for _ in range(num_layers)]
    self.dropout = tf.keras.layers.Dropout(dropout_rate)

  def call(self, x):
    # `x` is token-IDs shape: (batch, seq_len)
    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.

    # Add dropout.
    x = self.dropout(x)

    for i in range(self.num_layers):
      x = self.enc_layers[i](x)

    return x  # Shape `(batch_size, seq_len, d_model)`.
#----------------------------------------------

"""Test the encoder:"""

# Instantiate the encoder.
sample_encoder = Encoder(num_layers=num_layers,
                         d_model=d_model,
                         num_heads=num_heads,
                         dff=dff,
                         vocab_size=context_vocab_size)

sample_encoder_output = sample_encoder(ct, training=False)

# Print the shape.
print("Context shape",ct.shape)
print("Shape sample_encoder_output",sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`.


"""### The decoder layer

The decoder's stack is slightly more complex, with each `DecoderLayer` containing a `CausalSelfAttention`, a `CrossAttention`, and a `FeedForward` layer:
"""

class DecoderLayer(tf.keras.layers.Layer):
  def __init__(self,
               *,
               d_model,
               num_heads,
               dff,
               dropout_rate=0.1):
    super(DecoderLayer, self).__init__()

    self.causal_self_attention = CausalSelfAttention(
        num_heads=num_heads,
        key_dim=d_model,
        dropout=dropout_rate)

    self.cross_attention = CrossAttention(
        num_heads=num_heads,
        key_dim=d_model,
        dropout=dropout_rate)

    self.ffn = FeedForward(d_model, dff)

  def call(self, x, context):
    x = self.causal_self_attention(x=x)
    x = self.cross_attention(x=x, context=context)

    # Cache the last attention scores for plotting later
    self.last_attn_scores = self.cross_attention.last_attn_scores

    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.
    return x
#----------------------------------------------

"""Test the decoder layer:"""

sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)

sample_decoder_layer_output = sample_decoder_layer(
    x=target_emb, context=context_emb)

print(target_emb.shape)
print(context_emb.shape)
print(sample_decoder_layer_output.shape)  # `(batch_size, seq_len, d_model)`


"""### The decoder

Similar to the `Encoder`, the `Decoder` consists of a `PositionalEmbedding`, and a stack of `DecoderLayer`s:

Define the decoder by extending `tf.keras.layers.Layer`:
"""

class Decoder(tf.keras.layers.Layer):
  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,
               dropout_rate=0.1):
    super(Decoder, self).__init__()

    self.d_model = d_model
    self.num_layers = num_layers

    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,
                                             d_model=d_model)
    self.dropout = tf.keras.layers.Dropout(dropout_rate)
    self.dec_layers = [
        DecoderLayer(d_model=d_model, num_heads=num_heads,
                     dff=dff, dropout_rate=dropout_rate)
        for _ in range(num_layers)]

    self.last_attn_scores = None

  def call(self, x, context):
    # `x` is token-IDs shape (batch, target_seq_len)
    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)

    x = self.dropout(x)

    for i in range(self.num_layers):
      x  = self.dec_layers[i](x, context)

    self.last_attn_scores = self.dec_layers[-1].last_attn_scores

    # The shape of x is (batch_size, target_seq_len, d_model).
    return x
#----------------------------------------------

"""Test the decoder:"""

# Instantiate the decoder.
sample_decoder = Decoder(num_layers=num_layers,
                         d_model=d_model,
                         num_heads=num_heads,
                         dff=dff,
                         vocab_size=target_vocab_size)

output = sample_decoder(x=tg,context=context_emb)

# Print the shapes.
print("Shape target:",tg.shape)
print("Shape Context embedding:",context_emb.shape)
print("Shape sample decoder:",output.shape)

sample_decoder.last_attn_scores.shape  # (batch, heads, target_seq, input_seq)

"""Having created the Transformer encoder and decoder, it's time to build the Transformer model and train it.


## The Transformer

You now have `Encoder` and `Decoder`. To complete the `Transformer` model, you need to put them together and add a final linear (`Dense`) layer which converts the resulting vector at each location into output token probabilities.

The output of the decoder is the input to this final linear layer.

Create the `Transformer` by extending `tf.keras.Model`:
"""

class Transformer(tf.keras.Model):
  def __init__(self, *, num_layers, d_model, num_heads, dff,
               input_vocab_size, target_vocab_size, dropout_rate=0.1):
    super().__init__()
    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,
                           num_heads=num_heads, dff=dff,
                           vocab_size=input_vocab_size,
                           dropout_rate=dropout_rate)

    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,
                           num_heads=num_heads, dff=dff,
                           vocab_size=target_vocab_size,
                           dropout_rate=dropout_rate)

    self.final_layer = tf.keras.layers.Dense(target_vocab_size)

  def call(self, inputs):
    # To use a Keras model with `.fit` you must pass all your inputs in the
    # first argument.
    context, x  = inputs

    context = self.encoder(context)  # (batch_size, context_len, d_model)

    x = self.decoder(x, context)  # (batch_size, target_len, d_model)

    # Final linear layer output.
    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)

    try:
      # Drop the keras mask, so it doesn't scale the losses/metrics.
      # b/250038731
      del logits._keras_mask
    except AttributeError:
      pass

    # Return the final output and the attention weights.
    return logits
#----------------------------------------------

"""### Hyperparameters

To keep this example small and relatively fast, the number of layers (`num_layers`), the dimensionality of the embeddings (`d_model`), and the internal dimensionality of the `FeedForward` layer (`dff`) have been reduced.

The base model described in the original Transformer paper used `num_layers=6`, `d_model=512`, and `dff=2048`.

The number of self-attention heads remains the same (`num_heads=8`).

Instantiate the `Transformer` model:
"""

transformer = Transformer(
    num_layers=num_layers,
    d_model=d_model,
    num_heads=num_heads,
    dff=dff,
    input_vocab_size=context_vocab_size,
    target_vocab_size=target_vocab_size,
    dropout_rate=dropout_rate)

"""Test it:"""

output = transformer((ct, tg))

print(tg.shape)
print(ct.shape)
print(output.shape)

attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores
print(attn_scores.shape)  # (batch, heads, target_seq, input_seq)

"""Print the summary of the model:"""

transformer.summary()



"""## Training

### Set up the optimizer

Use the Adam optimizer with a custom learning rate scheduler according to the formula in the original Transformer [paper](https://arxiv.org/abs/1706.03762).

$$\Large{lrate = d_{model}^{-0.5} * \min(step{\_}num^{-0.5}, step{\_}num \cdot warmup{\_}steps^{-1.5})}$$
"""

class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
  def __init__(self, d_model, warmup_steps=4000):
    super().__init__()
    d_model = d_model
    warmup_steps = warmup_steps

    self.d_model = d_model
    self.d_model = tf.cast(self.d_model, tf.float32)

    self.warmup_steps = warmup_steps

  def __call__(self, step):
    step = tf.cast(step, dtype=tf.float32)
    arg1 = tf.math.rsqrt(step)
    arg2 = step * (self.warmup_steps ** -1.5)
    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)

  def get_config(self):
    return {
        "d_model": float(self.d_model),
        "warmup_steps": float(self.warmup_steps),
    }

  @classmethod
  def from_config(cls, config):
      return cls(**config)
#----------------------------------------------

"""Instantiate the optimizer (in this example it's `tf.keras.optimizers.Adam`):"""

learning_rate = CustomSchedule(d_model)

optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,
                                     epsilon=1e-9)

"""### Set up the loss and metrics

Since the target sequences are padded, it is important to apply a padding mask when calculating the loss. Use the cross-entropy loss function (`tf.keras.losses.SparseCategoricalCrossentropy`):
"""

def masked_loss(label, pred):
  mask = label != 0
  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')
  loss = loss_object(label, pred)

  mask = tf.cast(mask, dtype=loss.dtype)
  loss *= mask

  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)
  return loss
#----------------------------------------------

def masked_accuracy(label, pred):
  pred = tf.argmax(pred, axis=2)
  label = tf.cast(label, pred.dtype)
  match = label == pred

  mask = label != 0

  match = match & mask

  match = tf.cast(match, dtype=tf.float32)
  mask = tf.cast(mask, dtype=tf.float32)
  return tf.reduce_sum(match)/tf.reduce_sum(mask)
#----------------------------------------------

"""### Train the model

With all the components ready, configure the training procedure using `model.compile`, and then run it with `model.fit`:

Note: This takes about an hour to train in Colab.
"""

checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=save_weights_only,  # Save only the model's weights
    save_best_only=save_best_only,  # Save only the best model based on a monitored metric
    monitor='val_loss',  # Metric to monitor for determining the best model
    mode='min',  # 'min' or 'max' depending on the monitored metric
    save_freq=save_freq,  # Save checkpoints after every x epochs
)

if pretrained_weights != None:
  print(f"Loading pretrained weights from {pretrained_weights}")
  transformer.load_weights(pretrained_weights)

transformer.compile(
    loss=masked_loss,
    optimizer=optimizer,
    metrics=[masked_accuracy])

transformer.fit(train_batches,
                epochs=epochs,
                validation_data=val_batches,
                callbacks=[checkpoint_callback]
                )

#save the model architecture
transformer.save(filepath=f'{model_dir}/saved_keras/',save_format='keras')

#load the best weights
if (save_weights_only == True and save_best_only == True):
  transformer.load_weights(checkpoint_filepath)


"""## Run inference"""

class Translator(tf.Module):
  def __init__(self, context_tokenizers, target_tokenizers, transformer):
    self.context_tokenizers = context_tokenizers
    self.target_tokenizers = target_tokenizers
    self.transformer = transformer

  def __call__(self, sentence, max_length=MAX_TOKENS): #max_length=MAX_TOKENS
    # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.
    assert isinstance(sentence, tf.Tensor)
    if len(sentence.shape) == 0:
      sentence = sentence[tf.newaxis]

    sentence = tokenize(sentence,self.context_tokenizers).to_tensor()

    encoder_input = sentence

    # As the output language is English, initialize the output with the
    # English `[START]` token.

    start_end = tokenize('',self.target_tokenizers)[0]
    start = start_end[0][tf.newaxis]
    end = start_end[-1][tf.newaxis]

    # `tf.TensorArray` is required here (instead of a Python list), so that the
    # dynamic-loop can be traced by `tf.function`.
    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)
    output_array = output_array.write(0, start)

    for i in tf.range(max_length):
      output = tf.transpose(output_array.stack())
      predictions = self.transformer([encoder_input, output], training=False)

      # Select the last token from the `seq_len` dimension.
      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.

      predicted_id = tf.argmax(predictions, axis=-1)

      # Concatenate the `predicted_id` to the output which is given to the
      # decoder as its input.
      output_array = output_array.write(i+1, predicted_id[0])

      if predicted_id == end:
        break

    output = tf.transpose(output_array.stack())
    # The output shape is `(1, tokens)`.

    text = self.target_tokenizers.detokenize(output)

    tokens = tf.gather(target_vocab, output)

    # `tf.function` prevents us from using the attention_weights that were
    # calculated on the last iteration of the loop.
    # So, recalculate them outside the loop.
    self.transformer([encoder_input, output[:,:-1]], training=False)
    attention_weights = self.transformer.decoder.last_attn_scores

    joined_text = tf.strings.reduce_join(text[0][1:-1], separator=' ', axis=-1)
    return joined_text, tokens, attention_weights
#----------------------------------------------

"""Note: This function uses an unrolled loop, not a dynamic loop. It generates `MAX_TOKENS` on every call. Refer to the [NMT with attention](nmt_with_attention.ipynb) tutorial for an example implementation with a dynamic loop, which can be much more efficient.

Create an instance of this `Translator` class, and try it out a few times:
"""

translator = Translator(context_tokenizer, target_tokenizer, transformer)
print("Translator object created!")


"""
### Export the model
"""
print("Building the ExportTranslator class...")

class ExportTranslator(tf.Module):
  def __init__(self, translator):
    self.translator = translator

  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])
  def __call__(self, sentence):
    (result,
     tokens,
     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)

    return result
#----------------------------------------------

exp_translator = ExportTranslator(translator)

print("\nTesting inputs (test)....")

for i in range(10,15):
  print("Input:",context_test[i])
  print("Expected:",target_test[i])
  print("Output:",exp_translator(context_test[i]).numpy().decode())
  print()

print("\nTesting inputs (train)....")

for i in range(10,15):
  print("Input:",context_train[i])
  print("Expected:",target_train[i])
  print("Output:",exp_translator(context_train[i]).numpy().decode())
  print()

#"""### Saving the model"""
#MODEL_SAVED_FILES=f'{model_dir}/exported_translator'
#tf.saved_model.save(exp_translator, export_dir=MODEL_SAVED_FILES)
#print("Model saved in:",MODEL_SAVED_FILES)


"""### Calculate blue score for the model"""



def calc_bleu_score(context_test,target_test):
    # Calculate the BLEU score
    references = []
    translations = []
    count=1

    final_test = context_test

    print("Translatating ...")

    for chunk in final_test:
      predicted_translation = exp_translator(chunk)
      #print(count)
      count+=1
      translations.append(predicted_translation.numpy().decode().split())

    for t in target_test:
      references.append([t.split()])

    print("\nTranslations completed! working on blue scores ...")

    # Calculate the BLEU score
    bleu4_score = bleu.corpus_bleu(references, translations)
    bleu3_score = bleu.corpus_bleu(references, translations,weights=(0.33, 0.33, 0.33, 0))
    bleu2_score = bleu.corpus_bleu(references, translations,weights=(0.5, 0.5, 0, 0))
    bleu_score = bleu.corpus_bleu(references, translations,weights=(1, 0, 0, 0))

    return bleu_score,bleu2_score,bleu3_score,bleu4_score
#----------------------------------------------

context_bleu,target_bleu = shuffle_corpus(context_test,target_test)

#calculate blue score on test data
print("\nCalculating bleu score...")
bleu_score,bleu2_score,bleu3_score,bleu4_score = calc_bleu_score(context_bleu[:3000],target_bleu[:3000])
print("Bleu1 score:",bleu_score)
print("Bleu2 score:",bleu2_score)
print("Bleu3 score:",bleu3_score)
print("Bleu4 score:",bleu4_score)

import datetime
current_time = datetime.datetime.now()

#write result to file
with open(f"{model_dir}/bleu_score.txt",'w') as eval_file:
  eval_file.write(f"Time:{current_time}\n")
  eval_file.write(f"BLEU1 SCORE: {bleu_score}\n")
  eval_file.write(f"BLEU2 SCORE: {bleu2_score}\n")
  eval_file.write(f"BLEU3 SCORE: {bleu3_score}\n")
  eval_file.write(f"BLEU4 SCORE: {bleu4_score}\n\n")

print("\nExecution finished!")
