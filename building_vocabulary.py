# -*- coding: utf-8 -*-
"""Building Vocabulary.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X365005gC7RoeOYx-6mDmYs2_-meHgIR

## Building Vocabulary

### Wordpiece Tokenization using Bert Tokenizer
"""

import tensorflow as tf
import tensorflow_text
import os
import numpy as np
from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab

print("\n------------------BUILDING SUBWORD TOKENIZATION FOR ENGLISH AND KONKANI----------------------\n")

print("Please provide the following details accurately\n")

path_data = os.path.abspath(input("Relative path to the Dataset: "))

vocab_size = input("Enter max vocabulary size (press enter to leave to default 40,000): ").strip()

if vocab_size is None or vocab_size.isnumeric() == False:
    vocab_size = 40000
else:
    vocab_size = int(vocab_size)

path_vocab=os.path.abspath(path_data+'.vocab')

#load corpus from text file into a numpy list
def load_data(path):
    
    with open(path, encoding='utf-8') as file:
        lines = file.readlines()
        
        len_data=len(lines)
        context = np.empty(shape=len_data,dtype=object)
        i=0
        for line in lines:
            context[i] = line
            i+=1
    del lines
    return context
#--------------------------------------------

#load data
target_full_raw = load_data(path_data)

#convert to tensor
full_en = tf.data.Dataset.from_tensor_slices(target_full_raw)

bert_tokenizer_params=dict(lower_case=False)
reserved_tokens=["[PAD]", "[UNK]", "[START]", "[END]"]

bert_vocab_args = dict(
    # The target vocabulary size
    vocab_size = vocab_size,
    # Reserved tokens that must be included in the vocabulary
    reserved_tokens=reserved_tokens,
    # Arguments for `text.BertTokenizer`
    bert_tokenizer_params=bert_tokenizer_params,
    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`
    learn_params={},
)

# building the vocabulary

print("Building Vocabulary, This may take some time...")
en_vocab = bert_vocab.bert_vocab_from_dataset(
    full_en.batch(1000).prefetch(2),
    **bert_vocab_args
)
print("Vocabulary build complete!")

en_vocab_size = len(en_vocab)

print()
print("length of vocabulary:",en_vocab_size)
print("Examples:")
print(en_vocab[:10])
print(en_vocab[100:110])
print(en_vocab[1000:1010])
print(en_vocab[-10:])

#write vocab to file
def write_vocab_file(filepath, vocab):
  with open(filepath, 'w') as f:
    for token in vocab:
      print(token, file=f)

write_vocab_file(path_vocab, en_vocab)
